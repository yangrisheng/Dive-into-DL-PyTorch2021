{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"linear-regression-concise.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"SzUJ6Lggvoka"},"source":["# 线性回归的简洁实现\n",":label:`sec_linear_concise`\n","\n","在过去的几年里，出于对深度学习强烈的兴趣，许多公司、学者和业余爱好者开发了各种成熟的开源框架。通过这些框架可以自动化实现基于梯度的学习算法中重复性的工作。\n","在 :numref:`sec_linear_scratch` 中，我们只依赖了：（1）通过张量来进行数据存储和线性代数；（2）通过自动微分来计算梯度。实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用，现代深度学习库也为我们实现了这些组件。\n","\n","在本节中，我们将介绍如何(**通过使用深度学习框架来简洁地实现**) :numref:`sec_linear_scratch` 中的(**线性回归模型**)。\n","\n","## 生成数据集\n","\n","与 :numref:`sec_linear_scratch` 中类似，我们首先[**生成数据集**]。\n"]},{"cell_type":"code","metadata":{"id":"Di1cJlq-vqVF","executionInfo":{"status":"ok","timestamp":1628863791946,"user_tz":-480,"elapsed":2,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["pip install -U d2l"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"origin_pos":2,"tab":["pytorch"],"id":"g4giQDcrvokb","executionInfo":{"status":"ok","timestamp":1628863797189,"user_tz":-480,"elapsed":1057,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["import numpy as np\n","import torch\n","from torch.utils import data\n","from d2l import torch as d2l"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"origin_pos":4,"tab":["pytorch"],"id":"FGdQNKGyvokb","executionInfo":{"status":"ok","timestamp":1628863803531,"user_tz":-480,"elapsed":1109,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["true_w = torch.tensor([2, -3.4])\n","true_b = 4.2\n","features, labels = d2l.synthetic_data(true_w, true_b, 1000)#生成数据集，y=Xw+b"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":5,"id":"mifC2eZsvokb"},"source":["## 读取数据集\n","\n","我们可以[**调用框架中现有的API来读取数据**]。我们将 `features` 和 `labels` 作为API的参数传递，并在实例化数据迭代器对象时指定 `batch_size`。此外，布尔值 `is_train` 表示是否希望数据迭代器对象在每个迭代周期内打乱数据。\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["pytorch"],"id":"9pKHmMccvokb","executionInfo":{"status":"ok","timestamp":1628864455761,"user_tz":-480,"elapsed":1062,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["def load_array(data_arrays, batch_size, is_train=True):  #@save\n","    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n","    #is_train表示表示希望数据迭代器对象在每个迭代周期内打乱数据\n","    #data_arrays：表示可以传入多个矩阵，即是将features和labels作为参数\n","    #TensorDateset:把输入的两类数据进行一一对应；\n","    #DataLoader:重新排序\n","    dataset = data.TensorDataset(*data_arrays)\n","    #*可以对list解开入参，因为features和labels作为API参数传递\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","    #每次随机挑选batch_size个样本，shuffle意思是要不要打乱顺序"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"origin_pos":9,"tab":["pytorch"],"id":"D0lw8S-Lvokc","executionInfo":{"status":"ok","timestamp":1628864525979,"user_tz":-480,"elapsed":1043,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["batch_size = 10\n","data_iter = load_array((features, labels), batch_size)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":10,"id":"sH35mHnBvokc"},"source":["使用 `data_iter` 的方式与我们在 :numref:`sec_linear_scratch` 中使用 `data_iter` 函数的方式相同。为了验证是否正常工作，让我们读取并打印第一个小批量样本。\n","与 :numref:`sec_linear_scratch` 不同，这里我们使用 `iter` 构造Python迭代器，并使用 `next` 从迭代器中获取第一项。\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["pytorch"],"id":"QnL84mLUvokc","outputId":"d65d112e-db85-4fd0-903f-08a9bc9b7f56"},"source":["# 不能直接从data_iter中获得数据\n","next(iter(data_iter))#将data_iter用iter()函数转为迭代器，再使用next()函数从迭代器中获取数据"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[-0.7090, -0.5001],\n","         [-0.7020, -1.2230],\n","         [-1.3061, -1.7845],\n","         [ 0.3258,  0.9002],\n","         [ 0.3525, -0.1899],\n","         [ 2.2436,  0.1764],\n","         [ 0.6269,  0.9438],\n","         [ 0.9973,  0.2910],\n","         [ 0.3698,  0.0483],\n","         [-2.1677,  0.1078]]),\n"," tensor([[ 4.4798],\n","         [ 6.9499],\n","         [ 7.6255],\n","         [ 1.8088],\n","         [ 5.5744],\n","         [ 8.0814],\n","         [ 2.2349],\n","         [ 5.2224],\n","         [ 4.7888],\n","         [-0.5108]])]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"Rsp6gjJpvokc"},"source":["## 定义模型\n","\n","当我们在 :numref:`sec_linear_scratch` 中实现线性回归时，我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。但是，如果模型变得更加复杂，而且当你几乎每天都需要实现模型时，你会想简化这个过程。这种情况类似于从头开始编写自己的博客。做一两次是有益的、有启发性的，但如果每次你每需要一个博客就花一个月的时间重新发明轮子，那你将是一个糟糕的网页开发者。\n","\n","对于标准操作，我们可以[**使用框架的预定义好的层**]。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。我们首先定义一个模型变量`net`，它是一个 `Sequential` 类的实例。 `Sequential` 类为串联在一起的多个层定义了一个容器。当给定输入数据， `Sequential` 实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，依此类推。在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉标准的流水线。\n","\n","回顾 :numref:`fig_single_neuron` 中的单层网络架构，这一单层被称为 *全连接层*（fully-connected layer），因为它的每一个输入都通过矩阵-向量乘法连接到它的每个输出。\n"]},{"cell_type":"markdown","metadata":{"origin_pos":14,"tab":["pytorch"],"id":"9Oh_GEhEvokc"},"source":["在 PyTorch 中，全连接层在 `Linear` 类中定义。值得注意的是，我们将两个参数传递到 `nn.Linear` 中。第一个指定输入特征形状，即 2，第二个指定输出特征形状，输出特征形状为单个标量，因此为 1。\n"]},{"cell_type":"code","metadata":{"origin_pos":17,"tab":["pytorch"],"id":"3EbZZAkuvokc","executionInfo":{"status":"ok","timestamp":1628865721428,"user_tz":-480,"elapsed":408,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["# `nn` 是神经网络的缩写\n","from torch import nn\n","#Sequential 类为串联在一起的多个层定义了一个容器\n","#当给定输入数据， Sequential 实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，依此类推\n","net = nn.Sequential(nn.Linear(2, 1))#Linear中，第一个指定输入特征数，第二个指定输出数。\n","#输入维度是2，输出纬度是1；\n","#nn.Linear(2,1)可以理解为线性回归就是简单的单层神经网络，将其放在一个Sequential中"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":19,"id":"7nVf9qq-vokc"},"source":["## (**初始化模型参数**)\n","\n","在使用`net`之前，我们需要初始化模型参数。如在线性回归模型中的权重和偏置。\n","深度学习框架通常有预定义的方法来初始化参数。\n","在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零。\n"]},{"cell_type":"markdown","metadata":{"origin_pos":21,"tab":["pytorch"],"id":"Obova5Xwvokc"},"source":["正如我们在构造 `nn.Linear` 时指定输入和输出尺寸一样。现在我们直接访问参数以设定初始值。我们通过 `net[0]` 选择网络中的第一个图层，然后使用 `weight.data` 和 `bias.data` 方法访问参数。然后使用替换方法 `normal_` 和 `fill_` 来重写参数值。\n"]},{"cell_type":"code","metadata":{"origin_pos":24,"tab":["pytorch"],"id":"HNJMel7ivokc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628865857004,"user_tz":-480,"elapsed":324,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"91c54cc2-9f79-4a0f-9618-c78509e74189"},"source":["#net[0]表示使用网络中的第一个图层\n","#指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零\n","print(net[0].weight.data)\n","net[0].weight.data.normal_(0, 0.01)#使用正态分布替代data的值，均值为0，标准差为0.01\n","print(net[0].weight.data)\n","print(net[0].bias.data)\n","net[0].bias.data.fill_(0)\n","print(net[0].bias.data)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0008, -0.0002]])\n","tensor([[-0.0015, -0.0049]])\n","tensor([0.])\n","tensor([0.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":27,"tab":["pytorch"],"id":"VaLV5vfFvokc"},"source":["\n"]},{"cell_type":"markdown","metadata":{"origin_pos":29,"id":"oBY_t35Uvokd"},"source":["## 定义损失函数\n"]},{"cell_type":"markdown","metadata":{"origin_pos":31,"tab":["pytorch"],"id":"s4Vv5ZL_vokd"},"source":["[**计算均方误差使用的是`MSELoss`类，也称为平方 $L_2$ 范数**]。默认情况下，它返回所有样本损失的平均值。\n"]},{"cell_type":"code","metadata":{"origin_pos":34,"tab":["pytorch"],"id":"vXJaFA4yvokd","executionInfo":{"status":"ok","timestamp":1628867301481,"user_tz":-480,"elapsed":787,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["loss = nn.MSELoss()#计算均方误差使用的是MSELoss类，也称为平方𝐿2范数 mean-square error"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"V1n6ArE5vokd"},"source":["## 定义优化算法\n"]},{"cell_type":"markdown","metadata":{"origin_pos":38,"tab":["pytorch"],"id":"HKmZgqvXvokd"},"source":["小批量随机梯度下降算法是一种优化神经网络的标准工具，PyTorch 在 `optim` 模块中实现了该算法的许多变种。当我们(**实例化 `SGD` 实例**)时，我们要指定优化的参数（可通过 `net.parameters()` 从我们的模型中获得）以及优化算法所需的超参数字典。小批量随机梯度下降只需要设置 `lr`值，这里设置为 0.03。\n"]},{"cell_type":"code","metadata":{"origin_pos":41,"tab":["pytorch"],"id":"pOiWlhyZvokd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628866091569,"user_tz":-480,"elapsed":558,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"36e679dd-803b-481f-8915-ccaa5307a57e"},"source":["#PyTorch 在 optim 模块中实现了该算法的许多变种\n","#可通过 net.parameters() 从我们的模型中获得指定优化的参数\n","#SGD随机梯度下降\n","trainer = torch.optim.SGD(net.parameters(), lr=0.03)#net.parameters()包括w和b"],"execution_count":20,"outputs":[{"output_type":"stream","text":["<generator object Module.parameters at 0x7f680d8682d0>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":43,"id":"gdffEJtmvokd"},"source":["## 训练\n","\n","通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。\n","我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。\n","当我们需要更复杂的模型时，高级API的优势将大大增加。\n","当我们有了所有的基本组件，[**训练过程代码与我们从零开始实现时所做的非常相似**]。\n","\n","回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（`train_data`），不停地从中获取一个小批量的输入和相应的标签。对于每一个小批量，我们会进行以下步骤:\n","\n","* 通过调用 `net(X)` 生成预测并计算损失 `l`（正向传播）。\n","* 通过进行反向传播来计算梯度。\n","* 通过调用优化器来更新模型参数。\n","\n","为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。\n"]},{"cell_type":"code","metadata":{"origin_pos":45,"tab":["pytorch"],"id":"Wz9HssOVvokd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628867318063,"user_tz":-480,"elapsed":377,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"3873d65e-ea2f-4c80-f8f9-6fd5c31d571b"},"source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in data_iter:\n","        l = loss(net(X), y)#net(X)即为前向输出，y_hat\n","        trainer.zero_grad()#梯度清零\n","        l.backward()#等价于l.sum().backward()求和之后算梯度\n","        trainer.step()#调用优化算法进行模型更新\n","    l = loss(net(features), labels)\n","    print(f'epoch {epoch + 1}, loss {l:f}')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["epoch 1, loss 0.000103\n","epoch 2, loss 0.000103\n","epoch 3, loss 0.000103\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":47,"id":"wPDOQkuvvokd"},"source":["下面我们[**比较生成数据集的真实参数和通过有限数据训练获得的模型参数**]。\n","要访问参数，我们首先从 `net` 访问所需的层，然后读取该层的权重和偏置。\n","正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。\n"]},{"cell_type":"code","metadata":{"origin_pos":49,"tab":["pytorch"],"id":"FzYVrKl2vokd","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"error","timestamp":1628913338056,"user_tz":-480,"elapsed":11,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"3d6935f5-e3c5-4f1b-b292-6d0168ffb160"},"source":["w = net[0].weight.data\n","print('w的估计误差：', true_w - w.reshape(true_w.shape))\n","b = net[0].bias.data\n","print('b的估计误差：', true_b - b)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c9fc0b3a767a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w的估计误差：'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_w\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b的估计误差：'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_b\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"]}]},{"cell_type":"markdown","metadata":{"origin_pos":51,"id":"2_8d9Zr4vokd"},"source":["## 小结\n"]},{"cell_type":"markdown","metadata":{"origin_pos":53,"tab":["pytorch"],"id":"J36r6cqavokd"},"source":["* 我们可以使用 PyTorch 的高级 API更简洁地实现模型。\n","* 在 PyTorch 中，`data` 模块提供了数据处理工具，`nn` 模块定义了大量的神经网络层和常见损失函数。\n","* 我们可以通过`_` 结尾的方法将参数替换，从而初始化参数。\n"]},{"cell_type":"markdown","metadata":{"origin_pos":55,"id":"BGPIzwqzvoke"},"source":["## 练习\n"]},{"cell_type":"markdown","metadata":{"origin_pos":57,"tab":["pytorch"],"id":"0Eg48Upqvoke"},"source":["1. 如果我们用 `nn.MSELoss()` 替换 `nn.MSELoss(reduction='sum')`，为了使代码的行为相同，需要怎么更改学习速率？为什么？\n","1. 查看 PyTorch 文档，了解提供了哪些损失函数和初始化方法。用Huber损失来代替。\n","1. 你如何访问 `net[0].weight` 的梯度？\n","\n","[Discussions](https://discuss.d2l.ai/t/1781)\n"]}]}