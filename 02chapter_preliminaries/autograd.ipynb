{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"autograd.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"D3yjQNDJLyN3"},"source":["# 自动求导\n",":label:`sec_autograd`\n","\n","正如我们在 :numref:`sec_calculus`中所说的那样，求导是几乎所有深度学习优化算法的关键步骤。虽然求导的计算很简单，只需要一些基本的微积分，但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n","\n","深度学习框架通过自动计算导数，即*自动求导*（automatic differentiation），来加快这项工作。实际中，根据我们设计的模型，系统会构建一个*计算图*（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动求导使系统能够随后反向传播梯度。\n","这里，*反向传播*（backpropagate）只是意味着跟踪整个计算图，填充关于每个参数的偏导数。\n","\n","\n","## 一个简单的例子\n","\n","作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。首先，我们创建变量`x`并为其分配一个初始值。\n"]},{"cell_type":"code","metadata":{"origin_pos":2,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"YqHYvARILyN6","executionInfo":{"status":"ok","timestamp":1628433492359,"user_tz":-480,"elapsed":4304,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"31615965-8ec1-475e-90df-0aabc7117960"},"source":["import torch       #因此当输出不是标量时，调用.backwardI()就会出错。不是标量的时候要先进行求和，然后.sum().backward()\n","\n","x = torch.arange(4.0)\n","x"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 1., 2., 3.])"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"uREJN44RLyN8"},"source":["[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度。**]\n","重要的是，我们不会在每次对一个参数求导时都分配新的内存。因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。注意，标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"]},{"cell_type":"code","metadata":{"origin_pos":6,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"vAjSM9piLyN9","executionInfo":{"status":"ok","timestamp":1628433527583,"user_tz":-480,"elapsed":422,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"6fd8ea34-65ab-4e73-a490-41e32a971e13"},"source":["x.requires_grad_(True)  # 等价于 `x = torch.arange(4.0, requires_grad=True)`\n","print(x.grad) # 默认值是None"],"execution_count":3,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"_X0k4unBLyN9"},"source":["(**现在让我们计算$y$。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":10,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"qfWw3b5KLyN9","executionInfo":{"status":"ok","timestamp":1628433899491,"user_tz":-480,"elapsed":525,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"fe7059a7-be1f-4fc2-d8b8-8b31aca2f9ce"},"source":["y = 2 * torch.dot(x, x)\n","y\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(28., grad_fn=<MulBackward0>)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"5L1fD04sLyN-"},"source":["`x`是一个长度为4的向量，计算`x`和`x`的内积，得到了我们赋值给`y`的标量输出。接下来，我们可以[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"]},{"cell_type":"code","metadata":{"origin_pos":14,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"UuxW1sq0LyN-","executionInfo":{"status":"ok","timestamp":1628433966466,"user_tz":-480,"elapsed":582,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"e1c71b98-3ee2-4705-82a0-8d1e812d0cd8"},"source":["y.backward()\n","x.grad"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.,  4.,  8., 12.])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"jNcAbBTELyN_"},"source":["函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。让我们快速验证我们想要的梯度是否正确计算。\n"]},{"cell_type":"code","metadata":{"origin_pos":18,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"Ey8L4gjyLyN_","executionInfo":{"status":"ok","timestamp":1628433978326,"user_tz":-480,"elapsed":625,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"1af54d0b-2a60-4344-e66c-1c99fee60ca7"},"source":["x.grad == 4 * x"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"zP8H4V8WLyN_"},"source":["[**现在让我们计算`x`的另一个函数。**]\n"]},{"cell_type":"code","metadata":{"origin_pos":22,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"1Tz_aIy_LyOA","executionInfo":{"status":"ok","timestamp":1628434082713,"user_tz":-480,"elapsed":457,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"1d871d3b-8f6b-4614-e128-b11b79f37fec"},"source":["# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n","x.grad.zero_()\n","y = x.sum()\n","y.backward()\n","x.grad"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1.])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"70lo08KYLyOA"},"source":["## 非标量变量的反向传播\n","\n","当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n","\n","然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里(**，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":26,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"tok6Ff6OLyOA","executionInfo":{"status":"ok","timestamp":1628435796857,"user_tz":-480,"elapsed":689,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"95b19c0f-e659-4977-8c41-98f4d0461088"},"source":["# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n","x.grad.zero_()\n","y = x * x\n","# 等价于y.backward(torch.ones(len(x)))\n","y.sum().backward()\n","x.grad"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 2., 4., 6.])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"cU2GFyASLyOB"},"source":["## 分离计算\n","\n","有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n","例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n","现在，想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，我们希望将`y`视为一个常数，并且只考虑到`x`在`y`被计算后发挥的作用。\n","\n","在这里，我们可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，但丢弃计算图中如何计算`y`的任何信息。换句话说，梯度不会向后流经`u`到`x`。因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，而不是`z=x*x*x`关于`x`的偏导数。\n"]},{"cell_type":"code","metadata":{"id":"iS9a2l6AQxi_","executionInfo":{"status":"ok","timestamp":1628434689958,"user_tz":-480,"elapsed":453,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":[""],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"origin_pos":30,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"br_2LWZRLyOB","executionInfo":{"status":"ok","timestamp":1628435814557,"user_tz":-480,"elapsed":509,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"b2a2a349-943b-4e93-c32c-c73006cf7862"},"source":["x.grad.zero_()\n","\n","y = x * x\n","u = y.detach()#返回一个新的Variable，从当前计算图中分离下来的，但是仍指向原变量的存放位置,\n","              #不同之处只是requires_grad为false，得到的这个Variable永远不需要计算其梯度，不具有grad\n","print(u)\n","print(u.grad)\n","z = u * x#这里使用了u。u是没有grad的，x有。\n","print(z.sum())\n","z.sum().backward()\n","\n","\n","print(z)\n","x.grad == u"],"execution_count":38,"outputs":[{"output_type":"stream","text":["tensor([0., 1., 4., 9.])\n","None\n","tensor(36., grad_fn=<SumBackward0>)\n","tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"b17x3mhHLyOC"},"source":["由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，得到`y=x*x`关于的`x`的导数，这里是`2*x`。\n"]},{"cell_type":"code","metadata":{"origin_pos":34,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"TXudrCC9LyOC","executionInfo":{"status":"ok","timestamp":1628435823870,"user_tz":-480,"elapsed":429,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"609b6f91-e61f-4eb8-8b44-57756d85d4b2"},"source":["x.grad.zero_()\n","y.sum().backward()\n","x.grad == 2 * x"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"3xNpwblXLyOC"},"source":["## Python控制流的梯度计算\n","\n","使用自动求导的一个好处是，[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"]},{"cell_type":"code","metadata":{"origin_pos":38,"tab":["pytorch"],"id":"9NCFj0gmLyOC","executionInfo":{"status":"ok","timestamp":1628435843330,"user_tz":-480,"elapsed":417,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["def f(a):\n","    b = a * 2\n","    while b.norm() < 1000:\n","        b = b * 2\n","    if b.sum() > 0:\n","        c = b\n","    else:\n","        c = 100 * b\n","    return c"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":40,"id":"ZOGtzXumLyOD"},"source":["让我们计算梯度。\n"]},{"cell_type":"code","metadata":{"origin_pos":42,"tab":["pytorch"],"id":"cBBYjJ0SLyOD","executionInfo":{"status":"ok","timestamp":1628435850701,"user_tz":-480,"elapsed":627,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}}},"source":["a = torch.randn(size=(), requires_grad=True)\n","d = f(a)\n","d.backward()"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":44,"id":"ywu5bQtfLyOD"},"source":["我们现在可以分析上面定义的`f`函数。请注意，它在其输入`a`中是分段线性的。换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`。因此，`d/a`允许我们验证梯度是否正确。\n"]},{"cell_type":"code","metadata":{"origin_pos":46,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"itaOYNHLLyOD","executionInfo":{"status":"ok","timestamp":1628435888945,"user_tz":-480,"elapsed":533,"user":{"displayName":"risheng yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD2uQGddZHpZy-4_97SC35j07gF6vhPmfYeXT-=s64","userId":"11325606721622706007"}},"outputId":"b062365b-2d7c-41cf-f41e-479ebae072a4"},"source":["a.grad == d / a"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(True)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"origin_pos":48,"id":"6bqr9eoSLyOE"},"source":["## 小结\n","\n","* 深度学习框架可以自动计算导数。为了使用它，我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n","\n","## 练习\n","\n","1. 为什么计算二阶导数比一阶导数的开销要更大？\n","1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n","1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？此时，计算结果`f(a)`不再是标量。结果会发生什么？我们如何分析这个结果？\n","1. 重新设计一个求控制流梯度的例子。运行并分析结果。\n","1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"]},{"cell_type":"markdown","metadata":{"origin_pos":50,"tab":["pytorch"],"id":"cOw7Yw_ALyOE"},"source":["[Discussions](https://discuss.d2l.ai/t/1759)\n"]}]}